{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyv0isGdT0nL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as nnF\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import dill\n",
    "## This allows inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBO2ZGhJSDg5",
    "outputId": "cc366e79-dfc7-455c-97e5-dfa4ef1ae7f8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# For google drive connection in order to load the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "datasetRootPath = 'drive/MyDrive/Colab Notebooks/data/BreaKHis_v1'\n",
    "'''\n",
    "\n",
    "# For google drive connection in order to load the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "!ls drive/MyDrive/mock_pathology_GAN_transformed_img/*\n",
    "# To use stain normalized images\n",
    "#datasetRootPath = '/media/wenrui/c5fdcf91-411a-4936-a09e-8e6585f9fe28/data/BreakHis/breakhis-analysis/400xData_normalized'\n",
    "\n",
    "# To use non-normalized images\n",
    "#datasetRootPath = \"/media/wenrui/c5fdcf91-411a-4936-a09e-8e6585f9fe28/data/BreakHis/image_data/breast-cancer-ml-reu/data/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/\"\n",
    "\n",
    "datasetRootPath= \"drive/MyDrive/mock_pathology_GAN_transformed_img\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDbAZpfJTTnI"
   },
   "source": [
    "# Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2TYm1F9SfTY"
   },
   "outputs": [],
   "source": [
    "## General parameters\n",
    "verbose = False\n",
    "check = False\n",
    "\n",
    "fileName_BM     = ['_B_', '_M_']\n",
    "path_BM         = ['benign', 'malignant']\n",
    "magnifications  = ['40X', '100X', '200X', '400X']\n",
    "\n",
    "## Enumerates and dictionaries\n",
    "folds = ['fold-0', 'fold-1', 'fold-2', 'fold-3', 'fold-4']\n",
    "class_names = [ 'Benign', 'Malignant']\n",
    "\n",
    "## Execution parameters\n",
    "num_epochs = 30     # Epochs to be trained\n",
    "#lr=0.001            # Learning rate for the training\n",
    "lr=1e-3\n",
    "#momentum=0.9        # Momentum for the training\n",
    "returned='best'     # Model to be returned in the training. ['last' (default), 'best']\n",
    "\n",
    "\n",
    "#add device to cpu\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEqv64xtGNxu"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prk6Plrt81k6"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, names):\n",
    "        self.data=data\n",
    "        self.targets=targets\n",
    "        self.names=names\n",
    "\n",
    "    def __len__(self):\n",
    "        #return the number of data points\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        z = self.names[idx]\n",
    "        return x, y, z\n",
    "\n",
    "class DatasetDataContainer:\n",
    "    data = []\n",
    "    targets = []\n",
    "    names = []\n",
    "\n",
    "    def __init__(self, data, targets, names, test_size=0.1, k_splits=5, random_state=42, transformations=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.names = names\n",
    "        self.k_splits = k_splits\n",
    "        self.random_state = random_state\n",
    "        self.transformations = transformations\n",
    "        if self.transformations is None:\n",
    "            self.transformations = transforms.ToTensor()\n",
    "\n",
    "        # Generate train-test indexes\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=self.random_state)\n",
    "        self.train_index, self.test_index = list(sss.split(self.data, self.targets, self.names))[0]\n",
    "        \n",
    "        # Generate train-validation k-fold indexes\n",
    "        strtfdKFold = StratifiedKFold(n_splits=self.k_splits, shuffle=True, random_state=self.random_state)\n",
    "        self.kfold = list(strtfdKFold.split(self.data[self.train_index], self.targets[self.train_index], self.names[self.train_index]))\n",
    "    \n",
    "    def get_fold (self, fold, transformations):\n",
    "        train, val = self.kfold[fold]\n",
    "        fold_train = self.train_index[train]\n",
    "        fold_val = self.train_index[val]\n",
    "        #np.asarray(self.transformations(Image.open(fpath))) \n",
    "        train_dataset = MyDataset(np.asarray([np.asarray(transformations[0](x)) for x in self.data[fold_train]]), self.targets[fold_train], self.names[fold_train])\n",
    "        val_dataset = MyDataset(np.asarray([np.asarray(transformations[1](x)) for x in self.data[fold_val]]), self.targets[fold_val], self.names[fold_val])\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def get_fold_dataloaders(self, fold, transformations, batch_size=32, shuffle=True, num_workers=2):\n",
    "        train_dataset, val_dataset = self.get_fold(fold, transformations)\n",
    "        test_dataset = MyDataset(np.asarray([np.asarray(transformations[1](x)) for x in self.data[self.test_index]]), self.targets[self.test_index], self.names[self.test_index])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "        return {'train': train_dataloader, 'val': val_dataloader, 'test': test_dataloader}\n",
    "\n",
    "\n",
    "class BreakHistDataContainer:\n",
    "\n",
    "    pathLabelsList = ['benign', 'malignant']\n",
    "\n",
    "    def __init__(self, folderPath, magnification, test_size=0.1, k_splits=5, random_state=42, transformations=None):\n",
    "        self.transformations = transformations\n",
    "        if self.transformations is None:\n",
    "            self.transformations = transforms.ToTensor()\n",
    "        data, labels, names = self.read_images(folderPath, magnification)\n",
    "        self.dataContainer = DatasetDataContainer (data, labels, names, test_size, k_splits, random_state, transformations)\n",
    "    \n",
    "    def image_reader(self, fpath):\n",
    "        img = cv2.imread(fpath)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        #print(img.shape)\n",
    "        #print(ia.is_np_array(img))\n",
    " \n",
    "        return img\n",
    "    \n",
    "    def read_images (self, folderPath, magnification):\n",
    "        \n",
    "        # Take the paths of all images for that magnification in the given folder and subfolders and short them alphabetically\n",
    "        filepaths = glob.glob(folderPath + \"/**/*.png\", recursive=True)\n",
    "        filepaths = [fpath for fpath in filepaths if magnification in fpath]\n",
    "        self.filepaths = sorted(filepaths, key=lambda s: os.path.split(s)[-1])\n",
    "\n",
    "        # Read all images and extract their labels from the path\n",
    "        \n",
    "        # images = [np.asarray(self.transformations(Image.open(fpath))) for fpath in self.filepaths]\n",
    "        images = [np.asarray(self.image_reader(fpath)) for fpath in self.filepaths]\n",
    "        #images = list(self.filepaths)\n",
    "        labels = [0 if self.pathLabelsList[0] in fpath else 1 for fpath in self.filepaths]\n",
    "        names = [fpath.split(\"/\")[-1] for fpath in self.filepaths]\n",
    "\n",
    "        # Store images and labels\n",
    "        data = np.asarray(images, dtype=object)\n",
    "        #data = images\n",
    "        labels = np.asarray(labels)\n",
    "        names = np.asarray(names)\n",
    "\n",
    "        return data, labels, names\n",
    "    \n",
    "    def get_fold (self, fold, transformations):\n",
    "        return self.dataContainer.get_fold(fold, transformations)\n",
    "\n",
    "    def get_fold_dataloaders(self, fold, transformations, batch_size=32, num_workers=2):\n",
    "        return self.dataContainer.get_fold_dataloaders(fold, transformations, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PPoteMGUodY"
   },
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyP88qvnAyHi"
   },
   "outputs": [],
   "source": [
    "# Returns a collection of metrics for the labels and prediction given\n",
    "def get_metrics (y_true, y_pred):\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    roc_auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "    acc = metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    #return {'cm': [tp, fp, tn, fn], 'acc': acc, 'roc_auc': roc_auc, 'f1': f1, 'pr_auc': pr_auc}\n",
    "    return {'cm': [tp, fp, tn, fn], 'acc': acc, 'roc_auc': roc_auc, 'f1': f1, 'pr_auc': pr_auc,\\\n",
    "           'copy to google sheet': f\"{round(acc, 4)}, {round(roc_auc, 4)}, {round(f1, 4)}, {round(pr_auc, 4)},\\\n",
    "            {tp}, {fp}, {tn}, {fn}\"\n",
    "           }\n",
    "\n",
    "# Returns an average of the metrics collection given\n",
    "def average_metrics (metrics):\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    acc = 0\n",
    "    roc_auc = 0\n",
    "    f1 = 0\n",
    "    pr_auc = 0\n",
    "    s = len(metrics)\n",
    "\n",
    "    for execution in metrics:\n",
    "        tp += execution['cm'][0]\n",
    "        fp += execution['cm'][1]\n",
    "        tn += execution['cm'][2]\n",
    "        fn += execution['cm'][3]\n",
    "        acc += execution['acc']\n",
    "        roc_auc += execution['roc_auc']\n",
    "        f1 += execution['f1']\n",
    "        pr_auc += execution['pr_auc']\n",
    "\n",
    "    return {'cm': [tp/s, fp/s, tn/s, fn/s], 'acc': acc/s, 'roc_auc': roc_auc/s, 'f1': f1/s, 'pr_auc': pr_auc/s,\\\n",
    "           'copy to google sheet': f\"{round(acc/s, 4)}, {round(roc_auc/s, 4)}, {round(f1/s, 4)}, {round(pr_auc/s, 4)},\\\n",
    "            {tp/s}, {fp/s}, {tn/s}, {fn/s}\"\n",
    "           }\n",
    "\n",
    "# Return a confusion matrix of the trained model received as parameter.\n",
    "# dataloaders must be torch.utils.data.DataLoader\n",
    "# The confusion matrix is a list with this values [tp, fp, tn, fn]\n",
    "def get_evaluation_metrics (model, dataloader):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    sigm = torch.nn.Sigmoid()\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # iterate over test data\n",
    "        for inputs, labels, names in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs) # Feed Network\n",
    "            preds = sigm(outputs).cpu().reshape(-1).detach().numpy().round()\n",
    "            \n",
    "            y_pred.extend(preds) # Save Prediction\n",
    "            y_true.extend(labels.cpu()) # Save Truth\n",
    "    model.train(mode=was_training)\n",
    "    \n",
    "    return get_metrics (y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICQcpUK6SxzX"
   },
   "outputs": [],
   "source": [
    "def plot_epoch_evolution_ (loss_train_list, acc_train_list, acc_val_list, metric_train_list, \\\n",
    "                           metric_val_list, model_name, metric_name='acc', plot_size=6):\n",
    "    figures= 2 if metric_name=='acc' else 3\n",
    "    fig, ax = plt.subplots(1, figures, figsize=(plot_size*figures,plot_size))\n",
    "    ax[0].set_title('loss v.s. epoch',fontsize=16)\n",
    "    ax[0].plot(loss_train_list, '-b', label='training loss')\n",
    "    ax[0].set_xlabel('epoch',fontsize=16)\n",
    "    ax[0].legend(fontsize=16)\n",
    "    ax[0].grid(True)\n",
    "    ax[1].set_title('accuracy v.s. epoch',fontsize=16)\n",
    "    ax[1].plot(acc_train_list, '-b', label='training accuracy')\n",
    "    ax[1].plot(acc_val_list, '-g', label='validation accuracy')\n",
    "    ax[1].set_xlabel('epoch',fontsize=16)\n",
    "    ax[1].legend(fontsize=16)\n",
    "    ax[1].grid(True)\n",
    "    if figures==3:\n",
    "        ax[2].set_title(metric_name + ' v.s. epoch',fontsize=16)\n",
    "        ax[2].plot(metric_train_list, '-b', label='training ' + metric_name)\n",
    "        ax[2].plot(metric_val_list, '-g', label='validation ' + metric_name)\n",
    "        ax[2].set_xlabel('epoch',fontsize=16)\n",
    "        ax[2].legend(fontsize=16)\n",
    "        ax[2].grid(True)\n",
    "    \n",
    "    plt.savefig(f\"{datasetRootPath}/{model_name}_epoch_curves.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_epoch_evolution (epoch_evolution_list, model_name, metric_name='acc', plot_size=6):\n",
    "    loss_train_list=[]\n",
    "    acc_train_list=[]\n",
    "    acc_val_list=[]\n",
    "    metric_train_list=[]\n",
    "    metric_val_list=[]\n",
    "    for (train_loss, train_metrics, val_metrics) in epoch_evolution_list:\n",
    "        loss_train_list.append(train_loss)\n",
    "        acc_train_list.append(train_metrics['acc'])\n",
    "        acc_val_list.append(val_metrics['acc'])\n",
    "        metric_train_list.append(train_metrics[metric_name])\n",
    "        metric_val_list.append(val_metrics[metric_name])\n",
    "\n",
    "    plot_epoch_evolution_ (loss_train_list, acc_train_list, acc_val_list, metric_train_list, metric_val_list,\\\n",
    "                           model_name, metric_name, plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs1T5DH8SxyR"
   },
   "outputs": [],
   "source": [
    "## Training loop of of a pytorch model\n",
    "## It uses BCEWithLogitsLoss by default, so an activation functions is needed to be used in the output of the model\n",
    "## If returned is 'last' by default, returns the model trained in the last epoch, but if it's 'best'\n",
    "## It uses Adam optimizer by default\n",
    "## the model returned is the one with the best score in validation (F1 score)\n",
    "## lr and momentum are the learning rate and momentum of the scheduler\n",
    "## RETURNS a tuple (metrics, model)\n",
    "def train_model(model, dataloader_train, dataloader_val, model_name=None, num_epochs=25, returned='best', \\\n",
    "                criterion=None, optimizer=None, metric='f1', plot_evolution=True, verbose=False):\n",
    "    since = time.time()\n",
    "\n",
    "    # List to keep track of metrics evolution during training to plot at the end\n",
    "    epoch_tracking_list=[]\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    if criterion == None: \n",
    "        #criterion = torch.nn.BCELoss()\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    if optimizer == None:\n",
    "        #optimizer = optim.SGD(model.parameters())\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    sigm = torch.nn.Sigmoid()\n",
    "\n",
    "    metrics_best = {metric:0.0}\n",
    "    epoch_best = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        if verbose:\n",
    "            print('-' * 10)\n",
    "        epoch_since = time.time()\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for indx, (inputs, labels, names) in enumerate(dataloader_train):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Transforming loop\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs  = model(inputs)\n",
    "                loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "                preds = sigm(outputs).cpu().reshape(-1).detach().numpy().round()\n",
    "\n",
    "            # backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Result acumulation\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            y_pred.extend(preds) # Save Prediction\n",
    "            y_true.extend(labels.cpu()) # Save Truth\n",
    "\n",
    "        # Epoch evaluation\n",
    "        train_loss = running_loss / len(y_true)\n",
    "        train_metrics = get_metrics (y_true, y_pred)\n",
    "        val_metrics = get_evaluation_metrics(model, dataloader_val)\n",
    "        epoch_tracking_list.append((train_loss, train_metrics, val_metrics))\n",
    "\n",
    "        if verbose: \n",
    "            epoch_time_elapsed = time.time() - epoch_since\n",
    "            print('Train \\tLoss: {:.4f} \\t\\tAcc: {:.4f}'.format(train_loss, train_metrics['acc']))\n",
    "            print('Val \\tCM: {} \\tAcc: {:.4f} \\tROC auc: {:.4f} \\tF1: {:.4f} \\tP/R auc: {:.4f}'.format(val_metrics['cm'], val_metrics['acc'], val_metrics['roc_auc'], val_metrics['f1'], val_metrics['pr_auc']))\n",
    "            print('Time: {:.0f}m {:.0f}s'.format(epoch_time_elapsed // 60, epoch_time_elapsed % 60))\n",
    "            print('')\n",
    "\n",
    "        # deep copy the model\n",
    "        if val_metrics[metric] > metrics_best[metric] and returned == 'best':\n",
    "            metrics_best = val_metrics\n",
    "            model_best = copy.deepcopy(model)\n",
    "            epoch_best = epoch\n",
    "\n",
    "    if returned == 'best':\n",
    "        val_metrics = metrics_best\n",
    "        model = model_best\n",
    "        epoch = epoch_best\n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s. Epoch returned: {} ({})'.format(time_elapsed // 60, time_elapsed % 60, epoch, returned))\n",
    "    print(val_metrics)\n",
    "    \n",
    "    if plot_evolution:\n",
    "        print('\\n\\n')\n",
    "        plot_epoch_evolution (epoch_tracking_list, model_name, metric)\n",
    "\n",
    "    return (val_metrics, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaaTE2PAyH6u"
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_color_image(I):\n",
    "    I_max = I.max(axis=(0,1), keepdims=True)\n",
    "    I_min = I.min(axis=(0,1), keepdims=True)    \n",
    "    I = (I - I_min)/(I_max-I_min)\n",
    "    return I\n",
    "\n",
    "def explaination_single_models (img, label, modelList):\n",
    "\n",
    "    numImages = len(modelList)+1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('abs(dL/dx)')\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    ffig, ax = plt.subplots(1, 2, figsize=(12,10))\n",
    "    x=img.to(device)\n",
    "    ax[0].imshow(x.detach().cpu().numpy().squeeze().transpose(1,2,0))\n",
    "    ax[0].set_title('input image x, label='+str(label), fontsize=16)\n",
    "    #--------------------------------------------------\n",
    "    x=img.to(device)\n",
    "    x.requires_grad=True\n",
    "    z=model(x).view(1)\n",
    "    y=torch.tensor([label], dtype=x.dtype, device=device)\n",
    "    loss = nnF.binary_cross_entropy_with_logits(z, y)\n",
    "    loss.backward()\n",
    "    #--------------------------------------------------\n",
    "    y=y.item()\n",
    "    xx = x.detach().cpu().numpy().squeeze()\n",
    "    xx=xx.transpose(1,2,0)\n",
    "    x_grad=x.grad.data.detach().cpu().numpy().squeeze()\n",
    "    x_grad=x_grad.transpose(1,2,0)\n",
    "    x_grad=np.abs(x_grad).sum(axis=2)\n",
    "    xx = normalize_color_image(xx)\n",
    "    #--------------------------------------------------\n",
    "    ax[1].imshow(x_grad, cmap='gray', vmin=x_grad.min(), vmax=x_grad.max())\n",
    "    ax[1].set_title('abs(dL/dx)', fontsize=16)\n",
    "\n",
    "def explaination_multiple_models (img, label, modelList):\n",
    "\n",
    "    numImages = len(modelList)+1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('abs(dL/dx)')\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    fig, ax = plt.subplots(1, numImages, figsize=(24,20))\n",
    "    x=img.to(device)\n",
    "    ax[0].imshow(x.detach().cpu().numpy().squeeze().transpose(1,2,0))\n",
    "    ax[0].set_title('input image x, label='+str(label), fontsize=16)\n",
    "\n",
    "    i = 1\n",
    "    for (name, model) in modelList:\n",
    "        x=img.to(device)\n",
    "        x.requires_grad=True\n",
    "        z=model(x).view(1)\n",
    "        prediction = (z.data > 0).to(torch.int64).item()\n",
    "        y=torch.tensor([label], dtype=x.dtype, device=device)\n",
    "        loss = nnF.binary_cross_entropy_with_logits(z, y)\n",
    "        loss.backward()\n",
    "        #--------------------------------------------------\n",
    "        y=y.item()\n",
    "        xx = x.detach().cpu().numpy().squeeze()\n",
    "        xx=xx.transpose(1,2,0)\n",
    "        x_grad=x.grad.data.detach().cpu().numpy().squeeze()\n",
    "        x_grad=x_grad.transpose(1,2,0)\n",
    "        x_grad=np.abs(x_grad).sum(axis=2)\n",
    "        xx = normalize_color_image(xx)\n",
    "        ax[i].imshow(x_grad, cmap='gray', vmin=x_grad.min(), vmax=x_grad.max())\n",
    "        ax[i].set_title(name + ', pred='+str(prediction), fontsize=16)\n",
    "        #--------------------------------------------------\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26ZOvIuOGW1G"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGuMBUxT20bm"
   },
   "source": [
    "## IBN-ResNet / ResNet - get pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2IU7CNa20bn"
   },
   "outputs": [],
   "source": [
    "# Pretrained weights for all variation of IBN-Net can be found here https://github.com/XingangPan/IBN-Net/releases/tag/v1.0\n",
    "\n",
    "def get_pretrained_fc_linear (model_name, model_url=None, freeze = False):\n",
    "\n",
    "    if \"_ibn_\" not in model_name and model_url==None:\n",
    "      resname = model_name.split(\"_only\")[0]\n",
    "      weight_name = resname.replace(\"resnet\", \"ResNet\")\n",
    "      model = eval(f\"models.{resname}(weights = torchvision.models.{weight_name}_Weights.IMAGENET1K_V1)\")\n",
    "\n",
    "    else:\n",
    "      model = torch.hub.load('XingangPan/IBN-Net', model_name, pretrained=False)\n",
    "      \n",
    "      if freeze: # use pretrained weights\n",
    "          checkpoint = torch.hub.load_state_dict_from_url(model_url)#, \\\n",
    "                                                          #map_location='cpu')\n",
    "          # load weights\n",
    "          model.load_state_dict(checkpoint)\n",
    "\n",
    "     num_ftrs = model.fc.in_features\n",
    "     model.fc = nn.Sequential (    \n",
    "         nn.Linear(num_ftrs, 1)\n",
    "     )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4fO1-liwwTo"
   },
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj6gM2qd20bn"
   },
   "source": [
    "### Data will be only initialize (randomized) once to make sure that all the model using the same split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRxaF42u20bn"
   },
   "outputs": [],
   "source": [
    "data = BreakHistDataContainer(datasetRootPath, '400X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytH7TpCVFv15"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "## Transformations, usually model-dependentant\n",
    "crop_size = 460\n",
    "input_shape = 224\n",
    "#mean = [0.485, 0.456, 0.406]\n",
    "#std = [0.229, 0.224, 0.225]\n",
    "original data transformation\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Resize(input_shape),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean, std)\n",
    "    ])\n",
    "'''\n",
    "# using new data tranform by WH\n",
    "#! pip install imgaug\n",
    "\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.697759863, 0.68592817, 0.582152582], std=[0.095469999, 0.102921345, 0.139130713])\n",
    "mean = np.asarray([0.697759863, 0.68592817, 0.582152582])\n",
    "std = np.asarray([0.095469999, 0.102921345, 0.139130713])\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "iaa_state = 123\n",
    "\n",
    "'''\n",
    "Perform augmentation on training set only if there is augmentation\n",
    "Thus, we need two separate transformers for training and val-test datasets respectively\n",
    "'''\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "\n",
    "        \n",
    "    iaa.Sequential([iaa.size.Resize(224), iaa.SomeOf((3,7), [\n",
    "    iaa.Fliplr(0.5, random_state=iaa_state),\n",
    "    #iaa.CoarseDropout(0.1, size_percent=0.2, random_state=iaa_state),\n",
    "    iaa.Flipud(0.5, random_state=iaa_state),\n",
    "    iaa.OneOf([iaa.Affine(rotate=90, random_state=iaa_state),\n",
    "                iaa.Affine(rotate=180, random_state=iaa_state),\n",
    "                iaa.Affine(rotate=270, random_state=iaa_state)], random_state=iaa_state)])], random_order=True).augment_image,\n",
    "    #iaa.Multiply((0.8, 1.5), random_state=iaa_state),\n",
    "    #iaa.AdditiveGaussianNoise(scale=(0,0.2*255), per_channel=True, random_state=iaa_state),\n",
    "    #iaa.MultiplyHue((0.94,1.04), random_state=iaa_state),\n",
    "    #iaa.AddToBrightness((-20,64), random_state=iaa_state),\n",
    "    #iaa.MultiplySaturation((0.75,1.25), random_state=iaa_state),\n",
    "    #iaa.LinearContrast((0.25,1.75), random_state=iaa_state)])], random_order=True).augment_image,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    \n",
    "    ])\n",
    "\n",
    "'''\n",
    "NEED TO COMMENT OUT train_transforms = test_val_transforms PART IF AUG IS NEEDED!!!! \n",
    "'''\n",
    "\n",
    "\n",
    "test_val_transforms = transforms.Compose([\n",
    "\n",
    "        \n",
    "    iaa.Sequential([iaa.size.Resize(224)]).augment_image,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    \n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "#train_transforms = test_val_transforms\n",
    "\n",
    "#augmentation = \n",
    "\n",
    "#augmentation_valtest = iaa.Sequential([iaa.size.Resize(224)])\n",
    "\n",
    "#augmentation_valtest.augment_images(dataloaders['test'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLVnKX1G20bo"
   },
   "outputs": [],
   "source": [
    "dataloaders = data.get_fold_dataloaders(0, [train_transforms, test_val_transforms], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axCcP-5G20bo"
   },
   "source": [
    "### Save and load dataloaders to make sure that all the training are done with exact the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTXjuYm020bo",
    "outputId": "1824738b-84af-44a4-9f78-c00468bf70d4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "with open(f\"012323_NO_AUG_train_val_and_test_dataloaders_dill.pkl\", \"wb\") as f:\n",
    "    dill.dump(dataloaders, f, recurse=True)\n",
    "    \n",
    "print(\"Finished saving the dataloaders!\")\n",
    "\n",
    "with open(f\"012323_FLIP_ONLY_train_val_and_test_dataloaders_dill.pkl\", \"wb\") as f:\n",
    "    dill.dump(dataloaders, f, recurse=True)\n",
    "    \n",
    "print(\"Finished saving the dataloaders!\")\n",
    "'''\n",
    "#with open(f\"012323_train_val_and_test_dataloaders_dill.pkl\", \"rb\") as fp:\n",
    "    #dataloaders=dill.load(fp)\n",
    "    \n",
    "#print(\"Finished loading the dataloaders!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521,
     "referenced_widgets": [
      "2e0b7e4e35f344129b32fb8baff670ca",
      "22c3fa91fa274820b90eaab90ff4c9aa",
      "9f2b26b5e61d4954a3cbb49a32b4efa3",
      "23f2ce90c5374961b41255a09a5db8cc",
      "14e72addffa34b5a91b902c94ce70e9a",
      "126baeabddf24d79a9af1754fdde3ec6",
      "280eb9d0190f4d808443e8b39f989f96",
      "7bca68ac2b7d4b07acda19c1e71be772",
      "85560dbaa26348eba8923bea7f9b3f1c",
      "e04327cf389640878b2e37ab1f452eef",
      "5b717b738651450fb8402760d3afa670"
     ]
    },
    "id": "JOpb7mzgxDPO",
    "outputId": "681d485a-d9c2-4540-fb08-e1ee9afee199",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelList = []\n",
    "metricsList = []\n",
    "\n",
    "\n",
    "'''\n",
    "IBN_model_nameList = [\"resnet18_ibn_b_FLIP_ROTAT_ONLY\", \"resnet18_ibn_a_FLIP_ROTAT_ONLY\",\\\n",
    "                 \"resnet34_ibn_b_FLIP_ROTAT_ONLY\", \"resnet34_ibn_a_FLIP_ROTAT_ONLY\",\\\n",
    "                  \"resnet18_ibn_b_NO_AUG\", \"resnet18_ibn_a_NO_AUG\",\\\n",
    "                  \"resnet34_ibn_b_NO_AUG\", \"resnet34_ibn_a_NO_AUG\",\\\n",
    "                  \"resnet18_only_FLIP_ROTAT_ONLY\", \"resnet18_only_NO_AUG\",\\\n",
    "                 \"resnet34_only_FLIP_ROTAT_ONLY\", \"resnet34_only_NO_AUG\",\n",
    "                 \"resnet50_ibn_b_FLIP_ROTAT_ONLY\", \"resnet50_ibn_a_FLIP_ROTAT_ONLY\",\\\n",
    "                 \"resnet101_ibn_b_FLIP_ROTAT_ONLY\", \"resnet101_ibn_a_FLIP_ROTAT_ONLY\",\\\n",
    "                  \"resnet50_ibn_b_NO_AUG\", \"resnet50_ibn_a_NO_AUG\",\\\n",
    "                  \"resnet101_ibn_b_NO_AUG\", \"resnet101_ibn_a_NO_AUG\",\\\n",
    "                  \"resnet50_only_FLIP_ROTAT_ONLY\", \"resnet50_only_NO_AUG\",\\\n",
    "                 \"resnet101_only_FLIP_ROTAT_ONLY\", \"resnet101_only_NO_AUG\",\n",
    "                 ]\n",
    "'''\n",
    "IBN_model_nameList = [\"resnet18_only_NO_AUG\",\\\n",
    "                 \"resnet34_ibn_a_FLIP_ROTAT_ONLY\"]\n",
    "\n",
    "\n",
    "for IBN_model_name in IBN_model_nameList:\n",
    "        # set specific model to load\n",
    "    if \"resnet50_ibn_a\" in IBN_model_name:\n",
    "        IBN_model_url = \"https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet50_ibn_a-d9d0bb7b.pth\"\n",
    "        model_real_name = \"resnet50_ibn_a\"\n",
    "        \n",
    "    elif \"resnet50_ibn_b\" in IBN_model_name:\n",
    "        IBN_model_url = \"https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet50_ibn_b-9ca61e85.pth\"\n",
    "        model_real_name = \"resnet50_ibn_b\"\n",
    "        \n",
    "    elif \"resnet101_ibn_a\" in IBN_model_name:\n",
    "        IBN_model_url = \"https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet101_ibn_a-59ea0ac6.pth\"\n",
    "        model_real_name = \"resnet101_ibn_a\"\n",
    "        \n",
    "    elif \"resnet101_ibn_b\" in IBN_model_name:\n",
    "        IBN_model_url = \"https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet101_ibn_b-c55f6dba.pth\"\n",
    "        model_real_name = \"resnet101_ibn_b\"\n",
    "        \n",
    "    elif \"resnet18_ibn_a\" in IBN_model_name:\n",
    "        IBN_model_url = \"https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet18_ibn_a-2f571257.pth\"\n",
    "        model_real_name = \"resnet18_ibn_a\"\n",
    "        \n",
    "    elif \"resnet18_ibn_b\" in IBN_model_name:\n",
    "        IBN_model_url = \"https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet18_ibn_b-bc2f3c11.pth\"\n",
    "        model_real_name = \"resnet18_ibn_b\"\n",
    "        \n",
    "    elif \"resnet34_ibn_a\" in IBN_model_name:\n",
    "        IBN_model_url = \"https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet34_ibn_a-94bc1577.pth\"\n",
    "        model_real_name = \"resnet34_ibn_a\"\n",
    "        \n",
    "    elif \"resnet34_ibn_b\" in IBN_model_name:\n",
    "        IBN_model_url = \"https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet34_ibn_b-04134c37.pth\"\n",
    "        model_real_name = \"resnet34_ibn_b\"\n",
    "        \n",
    "    else:\n",
    "        IBN_model_url = None\n",
    "        model_real_name = IBN_model_name\n",
    "\n",
    "\n",
    "    model = get_pretrained_fc_linear(model_real_name, IBN_model_url, True)\n",
    "\n",
    "        \n",
    "    # choose either augmented or NOT augmented dataset    \n",
    "    if \"NO_AUG\" in IBN_model_name:\n",
    "        with open(f\"{datasetRootPath}/012323_NO_AUG_train_val_and_test_dataloaders_dill.pkl\", \"rb\") as fp:\n",
    "            dataloaders=dill.load(fp)\n",
    "    elif \"FLIP\" in IBN_model_name:\n",
    "        with open(f\"{datasetRootPath}/012323_FLIP_ONLY_train_val_and_test_dataloaders_dill.pkl\", \"rb\") as fp:\n",
    "            dataloaders=dill.load(fp)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # train the model and save to the list\n",
    "    num_epochs=99\n",
    "\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"Now processing {IBN_model_name}\")\n",
    "    pretrained_ResNet18_metrics, pretrained_ResNet18_model = train_model(model, dataloaders['train'], dataloaders['val'], criterion=criterion, \\\n",
    "                                                                         optimizer=optimizer, num_epochs=num_epochs, model_name=IBN_model_name)\n",
    "\n",
    "    modelList.append((IBN_model_name, pretrained_ResNet18_model))\n",
    "    metricsList.append((IBN_model_name, pretrained_ResNet18_metrics))\n",
    "    with open(f\"{datasetRootPath}/012423_modelList_and_metircsList.pkl\", \"wb\") as f:\n",
    "      dill.dump([modelList, metricsList], f, recurse=True)\n",
    "    print(f\"Finish saving modelList and metricsList till {IBN_model_name}\")\n",
    "    \n",
    "    \n",
    "print(\"FINISHED ALL PROCESSING!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Kh1XUYO20bp"
   },
   "source": [
    "### Test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TDc7rBa20br",
    "outputId": "656e240b-ca1d-432b-fa1e-0e9b9023132a"
   },
   "outputs": [],
   "source": [
    "for (name, model) in modelList:\n",
    "    print(f\"Now predicting on {name}\")\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sigm = torch.nn.Sigmoid()\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_names = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "    # iterate over test data\n",
    "        for inputs, labels, names in dataloaders['test']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_names.extend(names)\n",
    "\n",
    "            outputs = model(inputs) # Feed Network\n",
    "            preds = sigm(outputs).cpu().reshape(-1).detach().numpy().round()\n",
    "\n",
    "            y_pred.extend(preds) # Save Prediction\n",
    "            y_true.extend(labels.cpu()) # Save Truth\n",
    "    test_df = pd.DataFrame(zip(y_pred, y_true, y_names), columns=[\"y_pred\", \"y_true\", \"fname\"])\n",
    "    #print(test_df.head)\n",
    "    print(get_metrics (y_true, y_pred))\n",
    "    \n",
    "    with open(f\"{datasetRootPath}/{name}_trained_model_and_test_results_dill.pkl\", \"wb\") as f:\n",
    "      dill.dump([model, test_df], f, recurse=True)\n",
    "    print(\"Finished saving the results!\")\n",
    "    \n",
    "print(\"Finished all predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce02fCNL8Yda"
   },
   "source": [
    "### Get test statistics on wrongly classified test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCQcMxjp8eZN",
    "outputId": "15ae4ce3-1721-410b-de80-5bef7fb6328e"
   },
   "outputs": [],
   "source": [
    "# Get list of all pkl files\n",
    "list_all_pkl_files = glob.glob(f\"{datasetRootPath}/*trained_model_and_test_results*pkl\")\n",
    "list_all_pkl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WCI2lnq20bt",
    "outputId": "2397e1d5-3c00-405a-8d0f-4e23f2697552"
   },
   "outputs": [],
   "source": [
    "FP_dict = dict()\n",
    "FN_dict = dict()\n",
    "final_pd = pd.DataFrame()\n",
    "count = 0\n",
    "for pkl_path in list_all_pkl_files:\n",
    "    \n",
    "    name = pkl_path.split(\"/\")[-1].split(\"_trained_model_and_test_results_dill.pkl\")[0]\n",
    "    print(f\"Working on {name}\")\n",
    "    try:\n",
    "      with open(f\"{pkl_path}\", \"rb\") as f:\n",
    "          model, test_df = dill.load(f)\n",
    "    except:\n",
    "      continue\n",
    "    count += 1\n",
    "    #print(test_df.sort_values(\"fname\"))\n",
    "    new_test_df = test_df.rename({\"y_pred\":f\"{name}_y_pred\"},axis=1)\n",
    "    if final_pd.shape[0]==0:\n",
    "        \n",
    "        final_pd = new_test_df.set_index(\"fname\")[[\"y_true\", f\"{name}_y_pred\"]]\n",
    "        \n",
    "    else:\n",
    "        final_pd = final_pd.join(new_test_df[[\"fname\", f\"{name}_y_pred\"]].set_index(\"fname\"))\n",
    "    if len(FP_dict) == 0:\n",
    "        FP_dict = dict(zip(test_df.fname, [0]*len(test_df.fname)))\n",
    "        FN_dict = dict(zip(test_df.fname, [0]*len(test_df.fname)))\n",
    "    #make sure the test dataset is the same for all the experiments\n",
    "    assert len((set(test_df.fname)-set(FP_dict.keys())))==0\n",
    "    \n",
    "    for index, row in test_df.iterrows():\n",
    "        # False positive case\n",
    "        if row.y_pred > row.y_true:\n",
    "            FP_dict[row.fname]+=1\n",
    "        # False negative case\n",
    "        elif row.y_pred < row.y_true:\n",
    "            FN_dict[row.fname]+=1\n",
    "            \n",
    "\n",
    "# build the final Dataframe combined FP and FN rates\n",
    "FP_dict = pd.DataFrame(list(sorted(FP_dict.items(), key=lambda item: -item[1])), columns=[\"fname\", f\"Num_FP_in_total_{count}\"])\n",
    "FN_dict = pd.DataFrame(list(sorted(FN_dict.items(), key=lambda item: -item[1])), columns=[\"fname\", f\"Num_FN_in_total_{count}\"])\n",
    "            \n",
    "print(f\"Top 10 FPs:\\n {FP_dict.head(10)}\")\n",
    "print(f\"Top 10 FNs:\\n {FN_dict.head(10)}\")\n",
    "\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_Ar7lo7CnUF",
    "outputId": "b7eeb301-9db9-404a-854e-a66b10b2fb31"
   },
   "outputs": [],
   "source": [
    "final_pd.loc[\"SOB_B_F-14-25197-400-042.png\",].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipKNdDhw_K96"
   },
   "outputs": [],
   "source": [
    "FN_dict.to_csv(\"012523_num_FNs_all_400x_test_images.csv\", index=False)\n",
    "FP_dict.to_csv(\"012523_num_FPs_all_400x_test_images.csv\", index=False)\n",
    "final_pd.to_csv(\"012523_all_predictions_all_400x_test_iamges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwJKGVn__R_R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "126baeabddf24d79a9af1754fdde3ec6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14e72addffa34b5a91b902c94ce70e9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22c3fa91fa274820b90eaab90ff4c9aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_126baeabddf24d79a9af1754fdde3ec6",
      "placeholder": "​",
      "style": "IPY_MODEL_280eb9d0190f4d808443e8b39f989f96",
      "value": " 26%"
     }
    },
    "23f2ce90c5374961b41255a09a5db8cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e04327cf389640878b2e37ab1f452eef",
      "placeholder": "​",
      "style": "IPY_MODEL_5b717b738651450fb8402760d3afa670",
      "value": " 21.8M/83.2M [00:05&lt;00:18, 3.53MB/s]"
     }
    },
    "280eb9d0190f4d808443e8b39f989f96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e0b7e4e35f344129b32fb8baff670ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_22c3fa91fa274820b90eaab90ff4c9aa",
       "IPY_MODEL_9f2b26b5e61d4954a3cbb49a32b4efa3",
       "IPY_MODEL_23f2ce90c5374961b41255a09a5db8cc"
      ],
      "layout": "IPY_MODEL_14e72addffa34b5a91b902c94ce70e9a"
     }
    },
    "5b717b738651450fb8402760d3afa670": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7bca68ac2b7d4b07acda19c1e71be772": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85560dbaa26348eba8923bea7f9b3f1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9f2b26b5e61d4954a3cbb49a32b4efa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bca68ac2b7d4b07acda19c1e71be772",
      "max": 87289167,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_85560dbaa26348eba8923bea7f9b3f1c",
      "value": 22888448
     }
    },
    "e04327cf389640878b2e37ab1f452eef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
